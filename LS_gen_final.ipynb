{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swrp-h/ma_thesis/blob/main/LS_gen_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExAe1UCWn5E"
      },
      "source": [
        "# Final version of lexical simplification pipeline\n",
        " [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/)\n",
        "\n",
        "-Swarupa Hardikar\n",
        "\n",
        "MA Linguistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLjNFVAOGcsS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X_pz8MuY84Qh"
      },
      "outputs": [],
      "source": [
        "## installations for AL\n",
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "\n",
        "## installations for Orca\n",
        "!pip install auto-gptq\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VucO3HSMoJkz"
      },
      "outputs": [],
      "source": [
        "## Loading Alpaca LoRA\n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "model_al = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model_al = PeftModel.from_pretrained(model_al, \"tloen/alpaca-lora-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3_lzwcqermJ"
      },
      "outputs": [],
      "source": [
        "## Setup for AL\n",
        "## source for cell: https://colab.research.google.com/drive/1eWAmesrW99p7e1nah5bipn0zikMb8XYC\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egh3beCVRpW5"
      },
      "outputs": [],
      "source": [
        "## source for cell: https://colab.research.google.com/drive/1eWAmesrW99p7e1nah5bipn0zikMb8XYC\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model_al.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        return output.split(\"### Response:\")[1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYXde4bLiQlm"
      },
      "outputs": [],
      "source": [
        "evaluate(\"Instruction: \", prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inspiration for cell config from https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "model_name_or_path = \"TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GPTQ\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "model_oa = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=False,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)"
      ],
      "metadata": {
        "id": "ZCTPKhDPO1s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##prompt from https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML\n",
        "\n",
        "def generate_answer(prompt):\n",
        "    \"\"\"\n",
        "    Generate an answer for a given prompt using the loaded and defined model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    prompt_template = f'''<|prompter|>{prompt}<|endoftext|><|assistant|>\n",
        "    '''\n",
        "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "    output = model_oa.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
        "    return tokenizer.decode(output[0])"
      ],
      "metadata": {
        "id": "6zLZEliTy6eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iirAQAIg-fee"
      },
      "outputs": [],
      "source": [
        "## imports\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "## Setup for similarity model\n",
        "similarity_API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\"\n",
        "api_token = \"[TOKEN]\"\n",
        "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
        "\n",
        "## Setup for n-gram frequency model\n",
        "base_url = \"https://api.ngrams.dev\"\n",
        "corpus = \"eng\"\n",
        "\n",
        "def similarity_query(payload):\n",
        "    response = requests.post(similarity_API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\n",
        "class LexicalSimplification:\n",
        "    def __init__(self, data_file, model_name, prompt_template, output_file, ranker):\n",
        "        \"\"\"\n",
        "        Initialize the LexicalSimplification object.\n",
        "\n",
        "        Args:\n",
        "            data_file (str): Path to the data file containing sentences and target words.\n",
        "            model_name (str): Name or ID of the model to use for generating substitutes.\n",
        "            prompt_template (str): Template string for generating prompts, containing placeholders for sentence and word.\n",
        "            output_file (str): Path to the output file where the generated substitutes will be saved.\n",
        "            ranker (str): Method of substitute ranking.\n",
        "        \"\"\"\n",
        "        self.data = data_file\n",
        "        self.model = model_name\n",
        "        self.template = prompt_template\n",
        "        self.output = output_file\n",
        "        self.ranker = ranker\n",
        "\n",
        "    def read_data(self):\n",
        "        \"\"\"\n",
        "        Read the data file and return its contents.\n",
        "\n",
        "        Returns:\n",
        "            str: Contents of the data file.\n",
        "        \"\"\"\n",
        "        with open(self.data, 'r') as sf:\n",
        "            # Read and process the data file\n",
        "            data = sf.readlines()\n",
        "            # Additional processing or formatting as needed\n",
        "            return data\n",
        "\n",
        "    def gen_prompt(self, data):\n",
        "        \"\"\"\n",
        "        Generate a prompt by filling the template with the provided data.\n",
        "\n",
        "        Args:\n",
        "            data (str): Data containing sentences and target words in TSV format.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt generated using the template.\n",
        "        \"\"\"\n",
        "        sentence, word = data.strip().split('\\t')\n",
        "        prompt = self.template.replace('[WORD]', word).replace('[SENTENCE]', sentence)\n",
        "        return prompt\n",
        "\n",
        "    def answer_parser(self, answer, prompt):\n",
        "        \"\"\"\n",
        "        Parse the answer and extract the clean substitutes.\n",
        "\n",
        "        Args:\n",
        "            answer (str): Answer obtained from the model.\n",
        "\n",
        "        Returns:\n",
        "            list: List of clean substitutes.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        ## Catching noise from OA model\n",
        "        if \"<|assistant|>\" in answer:\n",
        "          answer = answer.split(\"<|assistant|>\")[1].replace(\"</s>\",\"\")\n",
        "        if \"Note:\" in answer:\n",
        "          answer = answer.split(\"Note:\")[0]\n",
        "\n",
        "        if \":\" in answer:\n",
        "            answer = answer.split(\":\")[-1]\n",
        "            clean_answer = answer.lower()\n",
        "        else:\n",
        "            clean_answer = answer.lower()\n",
        "\n",
        "        ## Cleaning the answer\n",
        "        clean_answer = re.sub(r'(?<![a-zA-Z])-|[^a-zA-Z\\s-]', '', clean_answer)\n",
        "        substitutes_list = clean_answer.strip().split()\n",
        "        ## Removing function words\n",
        "        substitutes_list = [word for word in substitutes_list if word not in stop_words and word not in prompt.lower().split()]\n",
        "        return substitutes_list\n",
        "\n",
        "    def generate_rank_substitutes(self):\n",
        "        \"\"\"\n",
        "        Generate substitutes for the target words in the data file using the model.\n",
        "        Save the generated substitutes to the output file in the desired format.\n",
        "        \"\"\"\n",
        "        # Reading the data\n",
        "        data = self.read_data()\n",
        "\n",
        "        # Iterating over the data and generate substitutes\n",
        "        with open(self.output, 'a') as self.output_file:\n",
        "          for i, line in enumerate(data):\n",
        "              sentence, complex_word = line.strip().split('\\t')\n",
        "              prompt = self.gen_prompt(line)\n",
        "              print(f\"{i+1})\")\n",
        "\n",
        "              ## Generation of substitutes\n",
        "              if self.model == \"al\":\n",
        "                answer = evaluate(\"Instruction: \", prompt)\n",
        "                print(answer)\n",
        "                substitutes_list = self.answer_parser(answer, prompt)\n",
        "\n",
        "              if self.model == \"oa\":\n",
        "                answer = generate_answer(prompt)\n",
        "                print(answer)\n",
        "                substitutes_list = self.answer_parser(answer, prompt)\n",
        "\n",
        "                ## Catching blank lists\n",
        "              if not substitutes_list:\n",
        "                substitutes_list.append(\"error\")\n",
        "\n",
        "                ## Removing duplicates\n",
        "              substitutes_set = set(substitutes_list)\n",
        "\n",
        "            ## Ranking the substitutes...\n",
        "              if self.ranker == \"similarity\":\n",
        "                word2score = defaultdict(float)\n",
        "\n",
        "                for substitute in substitutes_set:\n",
        "                    payload = {\n",
        "                        \"inputs\": {\n",
        "                            \"source_sentence\": sentence,\n",
        "                            \"sentences\": [sentence.replace(complex_word,substitute)]\n",
        "                        },\n",
        "                    \"options\": {\n",
        "                        \"wait_for_model\": True,\n",
        "                        \"use_cache\": True\n",
        "                    }\n",
        "                    }\n",
        "                    data = similarity_query(payload)\n",
        "                    print(data)\n",
        "                    word2score[substitute] = data[0]\n",
        "\n",
        "                print(word2score)\n",
        "                ranked_subs = [k for k, v in sorted(word2score.items(), key=lambda x: x[1], reverse=True)][:min(10, len(word2score))]\n",
        "\n",
        "              if self.ranker == \"frequency\":\n",
        "                ## needs streamlining...\n",
        "                word2score = defaultdict(float)\n",
        "\n",
        "                for substitute in substitutes_set:\n",
        "                    payload = {\n",
        "                        \"inputs\": {\n",
        "                            \"source_sentence\": sentence,\n",
        "                            \"sentences\": [sentence.replace(complex_word,substitute)]\n",
        "                        },\n",
        "                    \"options\": {\n",
        "                        \"wait_for_model\": True,\n",
        "                        \"use_cache\": True\n",
        "                    }\n",
        "                    }\n",
        "                    data = similarity_query(payload)\n",
        "                    word2score[substitute] = data[0]\n",
        "\n",
        "                print(word2score)\n",
        "                ranked_subs = [k for k, v in sorted(word2score.items(), key=lambda x: x[1], reverse=True)][:min(10, len(word2score))]\n",
        "\n",
        "                word2freq = defaultdict(int)\n",
        "                for substitute in ranked_subs:\n",
        "                    url = f\"{base_url}/{corpus}/search\"\n",
        "                    query_params = {\n",
        "                        \"query\": substitute\n",
        "                    }\n",
        "\n",
        "                    response = requests.get(url, params=query_params)\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        data = response.json()\n",
        "                        if \"ngrams\" in data and data[\"ngrams\"]:\n",
        "                            word2freq[substitute] = data[\"ngrams\"][0][\"absTotalMatchCount\"]\n",
        "                        else:\n",
        "                            word2freq[substitute] = 0\n",
        "\n",
        "                print(word2freq)\n",
        "\n",
        "                ranked_subs = [k for k, v in sorted(word2freq.items(), key=lambda x: x[1], reverse=True)][:min(10, len(word2freq))]\n",
        "\n",
        "              if self.ranker == \"nil\":\n",
        "                ranked_subs = substitutes_list[:10]\n",
        "\n",
        "              ranked_subs_str = '\\t'.join(ranked_subs)\n",
        "              output_line = sentence + '\\t' + complex_word + '\\t' + ranked_subs_str + '\\n'\n",
        "              self.output_file.write(output_line)\n",
        "              time.sleep(1)\n",
        "\n",
        "        print(\"Substitute ranking complete. Output saved in:\", self.output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hUkBndCCHvN1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Define the file paths and model details\n",
        "    data_file = \"[INPUT_PATH]\"\n",
        "    model_name = \"al\" # \"oa\"\n",
        "    # prompt_template = \"Context:'[SENTENCE]'\\n Given the above context, list 10 simpler substitutes for the word '[WORD]'.\"\n",
        "    prompt_template = '''Context: A local witness said a separate group of attackers disguised in burqas — the head-to-toe robes worn by conservative Afghan women — then tried to storm the compound.\\n\n",
        "                        Question: Given the above context, list ten alternative words\tfor “disguised” that are easier to understand. \\n\n",
        "                        Answer:\\n1. concealed\\n2. dressed\\n3. hidden\\n4. camouflaged\\n5. changed\\n6. covered\\n7. masked\\n8. unrecognizable\\n9. converted\\n10. impersonated\\n\\n\n",
        "                        Context: '[SENTENCE]'\\n\n",
        "                        Question: Given the above context, list ten alternatives for [WORD]' that are easier to understand. \\n\t\t\t\t'''\n",
        "    ranker = \"similarity\"\n",
        "    output_file = f\"[OUTPUT_PATH]/output_{model_name}_{ranker}_test_wcontext_1shot_p2.tsv\"\n",
        "\n",
        "    lexsimp = LexicalSimplification(data_file, model_name, prompt_template, output_file, ranker, threshold)\n",
        "\n",
        "    # Generate and rank substitutes and save them to the output file\n",
        "    lexsimp.generate_rank_substitutes()\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}